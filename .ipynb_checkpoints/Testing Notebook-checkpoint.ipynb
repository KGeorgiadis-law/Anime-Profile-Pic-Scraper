{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Profile Pic Scraper\n",
    "\n",
    "A bot to: \n",
    "1. Cycle through all the characters in https://myanimelist.net starting from https://myanimelist.net/character/1 ;\n",
    "2. For each character, navigate to its pictures page eg https://myanimelist.net/character/1/Spike_Spiegel/pictures * (as a side note, I could perhaps search instead for https://myanimelist.net/character/1/<???>/pictures) *\n",
    "3. Scrape all links to the profile pictures\n",
    "4. Create a new folder with the ID + Profile name of the Character (collaborator)\n",
    "5. Save all pictures in this new folder (collaborator)\n",
    "6. Repeat (collaborator)\n",
    "\n",
    "## How will it do each step?\n",
    "\n",
    "1. Character IDs are incremental - we can simply start from https://myanimelist.net/character/1 and then add 1 until we receive an error; As a failsafe, consider saving the enumerator in an outside text file.\n",
    "2. Two possible methods using bs4:\n",
    "    1. Scrape link to the pages by navigating to its position on the page\n",
    "    2. Search each `<a>` tag on the page for https://myanimelist.net/character/1/<???>/pictures\n",
    "3. Use bs4 to create a list of all the links * (idea, how about using a <a href=\"https://docs.python.org/3.3/tutorial/datastructures.html\"><strong>set rather than a list</strong></a>? Sets naturally check for duplicates and eliminate them - thus ensuring a picture is not downloaded twice). *\n",
    "4. It should be easy to scrape the name from either the original character page or the character page. Then use the os module to create a new folder with the id and the name.\n",
    "5. Cycle through the set created in step 3. and download the pictures. A good option to do this is `urllib.request.urlretrieve()` (<a href='https://stackoverflow.com/a/8286449'>see here for reference</a>)\n",
    "6. Print out confirmation message, add 1 to the enumerator, and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# !Py3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link_prefix = \"https://myanimelist.net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: \n",
    "* Cycle through all the characters in https://myanimelist.net starting from https://myanimelist.net/character/1\n",
    "* Character IDs are incremental - we can simply start from https://myanimelist.net/character/1 and then add 1 until we receive an error. \n",
    "* As a failsafe, **consider saving the enumerator in an outside text file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#while True: #uncomment this for production and indent everything below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#starting up\n",
    "character_id = 1\n",
    "\n",
    "#create a link\n",
    "character_page_url = \"https://myanimelist.net/character/\"+str(character_id)+\"/\"\n",
    "character_page_html = urlopen(character_page_url, timeout=30)\n",
    "#print(character_page_html.info()) #remove for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a Beautiful Soup object\n",
    "character_soup = BeautifulSoup(character_page_html, \"html.parser\")\n",
    "#print(soup.prettify()) #remove for production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "\n",
    "* For each character, navigate to its pictures page eg https://myanimelist.net/character/1/Spike_Spiegel/pictures\n",
    "* Two possible methods using bs4:\n",
    "    1. Scrape link to the pages by navigating to its position on the page\n",
    "    2. Search each `<a>` tag on the page for https://myanimelist.net/character/1/<???>/pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://myanimelist.net/character/1/Spike_Spiegel/pictures\n"
     ]
    }
   ],
   "source": [
    "# Method 1:\n",
    "\n",
    "#the link is enclosed in a div with the id content, so find that div\n",
    "div = character_soup.find(id=\"content\")\n",
    "\n",
    "#the link is invariably the first a tag in the div content - so find that\n",
    "#get('href') returns the href value of the <a> tag, i.e. the url we are looking for\n",
    "a_tag_href = div.a.get('href')\n",
    "\n",
    "#join the url with the prefix to make it valid\n",
    "pictures_url = link_prefix+a_tag_href\n",
    "print(pictures_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method 2: \n",
    "# couldn't get this to work - but this seems more complicated than method one, which seems to work just fine\n",
    "#a_tags =character_soup.find_all(\"a\")\n",
    "#for a_tag in a_tags:\n",
    "#    a_tag_link = a_tag.get('href')\n",
    "#    if a_tag_link != None and a_tag_link.startswith(\"https\"):\n",
    "#        split = a_tag_link.split(\"/\")\n",
    "#        if split[-1] == \"pictures\":\n",
    "#            print(a_tag_link) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:\n",
    "\n",
    "* open a soup object to the profile pictures page\n",
    "* Scrape all links to the profile pictures\n",
    "* Use bs4 to create a list of all the links * (idea, how about using a <a href=\"https://docs.python.org/3.3/tutorial/datastructures.html\"><strong>set rather than a list</strong></a>? Sets naturally check for duplicates and eliminate them - thus ensuring a picture is not downloaded twice). *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pictures_html = urlopen(pictures_url, timeout=30)\n",
    "pictures_soup = BeautifulSoup(pictures_html, \"html.parser\")\n",
    "#print(pictures_soup.prettify()) #remove for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pictures_div = pictures_soup.find(id=\"content\")\n",
    "\n",
    "tables_list = pictures_div.find_all(\"table\")\n",
    "picture_tags_set = set(tables_list[3].find_all(\"img\"))\n",
    "picture_urls = set()\n",
    "for picture in pictures_set:\n",
    "    picture_url = picture.get(\"src\")\n",
    "    picture_urls.add(picture_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Saving\n",
    "\n",
    "As per instructions:\n",
    "\"and you collect the image urls, make a dict from url to character ids, and url to character name\"\n",
    "\n",
    "hint given:\n",
    "\n",
    "`unique_char_names = set(blablabla)\n",
    "char_names_to_char_ids = dict(zip(unique_char_names, [i for i in range(len(unique_char_names)])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get character name\n",
    "character_name = pictures_soup.find(\"meta\", property=\"og:title\").get(\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make dict from url to character id\n",
    "url_to_char_id = dict()\n",
    "url_to_char_name = dict()\n",
    "\n",
    "#if there is a better way to do this... I don't know what it is\n",
    "for url in picture_urls:\n",
    "    url_to_char_id[url] = character_id\n",
    "    url_to_char_name[url] = character_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://myanimelist.cdn-dena.com/images/characters/16/312092.jpg: 1\n",
      "https://myanimelist.cdn-dena.com/images/characters/10/34138.jpg: 1\n",
      "https://myanimelist.cdn-dena.com/images/characters/8/310895.jpg: 1\n",
      "https://myanimelist.cdn-dena.com/images/characters/10/331008.jpg: 1\n",
      "https://myanimelist.cdn-dena.com/images/characters/10/325264.jpg: 1\n",
      "https://myanimelist.cdn-dena.com/images/characters/11/34139.jpg: 1\n",
      "https://myanimelist.cdn-dena.com/images/characters/13/310897.jpg: 1\n",
      "https://myanimelist.cdn-dena.com/images/characters/13/324139.jpg: 1\n",
      "https://myanimelist.cdn-dena.com/images/characters/7/311332.jpg: 1\n",
      "https://myanimelist.cdn-dena.com/images/characters/12/324293.jpg: 1\n",
      "https://myanimelist.cdn-dena.com/images/characters/5/34140.jpg: 1\n",
      "https://myanimelist.cdn-dena.com/images/characters/9/78620.jpg: 1\n",
      "https://myanimelist.cdn-dena.com/images/characters/4/324220.jpg: 1\n",
      "https://myanimelist.cdn-dena.com/images/characters/3/328601.jpg: 1\n",
      "https://myanimelist.cdn-dena.com/images/characters/4/50197.jpg: 1\n",
      "https://myanimelist.cdn-dena.com/images/characters/16/312092.jpg: Spike Spiegel\n",
      "https://myanimelist.cdn-dena.com/images/characters/10/34138.jpg: Spike Spiegel\n",
      "https://myanimelist.cdn-dena.com/images/characters/8/310895.jpg: Spike Spiegel\n",
      "https://myanimelist.cdn-dena.com/images/characters/10/331008.jpg: Spike Spiegel\n",
      "https://myanimelist.cdn-dena.com/images/characters/10/325264.jpg: Spike Spiegel\n",
      "https://myanimelist.cdn-dena.com/images/characters/11/34139.jpg: Spike Spiegel\n",
      "https://myanimelist.cdn-dena.com/images/characters/13/310897.jpg: Spike Spiegel\n",
      "https://myanimelist.cdn-dena.com/images/characters/13/324139.jpg: Spike Spiegel\n",
      "https://myanimelist.cdn-dena.com/images/characters/7/311332.jpg: Spike Spiegel\n",
      "https://myanimelist.cdn-dena.com/images/characters/12/324293.jpg: Spike Spiegel\n",
      "https://myanimelist.cdn-dena.com/images/characters/5/34140.jpg: Spike Spiegel\n",
      "https://myanimelist.cdn-dena.com/images/characters/9/78620.jpg: Spike Spiegel\n",
      "https://myanimelist.cdn-dena.com/images/characters/4/324220.jpg: Spike Spiegel\n",
      "https://myanimelist.cdn-dena.com/images/characters/3/328601.jpg: Spike Spiegel\n",
      "https://myanimelist.cdn-dena.com/images/characters/4/50197.jpg: Spike Spiegel\n"
     ]
    }
   ],
   "source": [
    "for key, value in url_to_char_id.items():\n",
    "    print(\"{}: {}\".format(key, value))\n",
    "for key, value in url_to_char_name.items():\n",
    "    print(\"{}: {}\".format(key, value))    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

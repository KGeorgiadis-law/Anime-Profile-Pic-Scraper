{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Profile Pic Scraper\n",
    "\n",
    "A bot to: \n",
    "1. Cycle through all the characters in https://myanimelist.net starting from https://myanimelist.net/character/1 ;\n",
    "2. For each character, navigate to its pictures page eg https://myanimelist.net/character/1/Spike_Spiegel/pictures * (as a side note, I could perhaps search instead for https://myanimelist.net/character/1/<???>/pictures) *\n",
    "3. Scrape all links to the profile pictures\n",
    "4. Create a new folder with the ID + Profile name of the Character\n",
    "5. Save all pictures in this new folder\n",
    "6. Repeat\n",
    "\n",
    "## How will it do each step?\n",
    "\n",
    "1. Character IDs are incremental - we can simply start from https://myanimelist.net/character/1 and then add 1 until we receive an error; As a failsafe, consider saving the enumerator in an outside text file.\n",
    "2. Two possible methods using bs4:\n",
    "    1. Scrape link to the pages by navigating to its position on the page\n",
    "    2. Search each `<a>` tag on the page for https://myanimelist.net/character/1/<???>/pictures\n",
    "3. Use bs4 to create a list of all the links * (idea, how about using a <a href=\"https://docs.python.org/3.3/tutorial/datastructures.html\"><strong>set rather than a list</strong></a>? Sets naturally check for duplicates and eliminate them - thus ensuring a picture is not downloaded twice). *\n",
    "4. It should be easy to scrape the name from either the original character page or the character page. Then use the os module to create a new folder with the id and the name.\n",
    "5. Cycle through the set created in step 3. and download the pictures. A good option to do this is `urllib.request.urlretrieve()` (<a href='https://stackoverflow.com/a/8286449'>see here for reference</a>)\n",
    "6. Print out confirmation message, add 1 to the enumerator, and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# !Py3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link_prefix = \"https://myanimelist.net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: \n",
    "* Cycle through all the characters in https://myanimelist.net starting from https://myanimelist.net/character/1\n",
    "* Character IDs are incremental - we can simply start from https://myanimelist.net/character/1 and then add 1 until we receive an error. \n",
    "* As a failsafe, **consider saving the enumerator in an outside text file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#while True: #uncomment this for production and indent everything below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: Fri, 02 Jun 2017 11:42:42 GMT\n",
      "Content-Type: text/html; charset=utf-8\n",
      "Transfer-Encoding: chunked\n",
      "Connection: close\n",
      "Server: Apache\n",
      "Set-Cookie: MALSESSIONID=d2gaaa41kv1hm6jnskueik1rq1; expires=Mon, 31-May-2027 11:42:42 GMT; Max-Age=315360000; path=/; secure; HttpOnly\n",
      "Set-Cookie: MALHLOGSESSID=eebe27405849f964728d354a8a2ab24f; expires=Wed, 01-Jun-2022 11:42:42 GMT; Max-Age=157680000; path=/\n",
      "Cache-Control: no-cache\n",
      "Vary: User-Agent,Accept-Encoding\n",
      "Strict-Transport-Security: max-age=63072000; includeSubDomains; preload\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#starting up\n",
    "character_id = 1000\n",
    "\n",
    "#create a link\n",
    "character_page_url = \"https://myanimelist.net/character/\"+str(character_id)+\"/\"\n",
    "character_page_html = urlopen(character_page_url, timeout=30)\n",
    "print(character_page_html.info()) #remove for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a Beautiful Soup object\n",
    "soup = BeautifulSoup(character_page_html, \"html.parser\")\n",
    "#print(soup.prettify()) #remove for production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "\n",
    "* For each character, navigate to its pictures page eg https://myanimelist.net/character/1/Spike_Spiegel/pictures\n",
    "* Two possible methods using bs4:\n",
    "    1. Scrape link to the pages by navigating to its position on the page\n",
    "    2. Search each `<a>` tag on the page for https://myanimelist.net/character/1/<???>/pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://myanimelist.net/character/1000/Chao_Lingshen/pictures\n"
     ]
    }
   ],
   "source": [
    "# Method 1:\n",
    "div = soup.find(id=\"content\")\n",
    "pictures_link = div.a.get('href')\n",
    "print(link_prefix+pictures_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method 2: # couldn't get this to work - but this seems more complicated than method one, which seems to work just fine\n",
    "a_tags = soup.find_all(\"a\")\n",
    "for a_tag in a_tags:\n",
    "    a_tag_link = a_tag.get('href')\n",
    "    if a_tag_link != None and a_tag_link.startswith(\"https\"):\n",
    "        split = a_tag_link.split(\"/\")\n",
    "        if split[-1] == \"pictures\":\n",
    "            print(a_tag_link) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
